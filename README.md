---

## Artefact Project

**Pipeline dâ€™ingestion et dâ€™orchestration des ventes**

---

## 1. PrÃ©sentation

Ce projet met en place un **pipeline de donnÃ©es industrialisÃ©** permettant lâ€™ingestion quotidienne de fichiers de ventes depuis un stockage objet **MinIO** vers une base de donnÃ©es **PostgreSQL**, selon un **modÃ¨le en Ã©toile** orientÃ© analytique.

Lâ€™ensemble est **conteneurisÃ© avec Docker** et orchestrÃ© via **Apache Airflow**, dans une logique proche dâ€™un environnement professionnel (sÃ©paration des responsabilitÃ©s, testabilitÃ©, extensibilitÃ©).

---

## 2. Objectifs du projet

* Automatiser lâ€™ingestion quotidienne des ventes
* Centraliser les donnÃ©es dans un schÃ©ma analytique (PostgreSQL)
* Mettre en place une orchestration robuste avec Airflow
* PrÃ©parer lâ€™Ã©volution vers des contrÃ´les qualitÃ© et du monitoring
* Appliquer les bonnes pratiques Data Engineering

---

## 3. Architecture globale

```text
MinIO (Object Storage)
        â”‚
        â–¼
Pipeline dâ€™ingestion Python
        â”‚
        â–¼
PostgreSQL (Data Warehouse â€“ schÃ©ma en Ã©toile)
        â”‚
        â–¼
Apache Airflow (orchestration & supervision)
```

---

## 4. Structure du projet

```text
Artefact_project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fashion_store_sales.csv
â”‚
â”œâ”€â”€ data_analysis/
â”‚   â”œâ”€â”€ analyse_exploratoire.ipynb
â”‚   â”œâ”€â”€ analyse_exploratoire.html
â”‚   â”œâ”€â”€ Classe_UML.png
â”‚   â””â”€â”€ data_modeling.md
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â””â”€â”€ plugins/
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â””â”€â”€ init/
â”‚   â”‚       â”œâ”€â”€ 01_schema.sql
â”‚   â”‚       â””â”€â”€ 02_tables.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ docker-compose-airflow.yml
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ minio_client.py
â”‚   â”œâ”€â”€ postgres_client.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ mock_sales_20250616.csv
â”‚   â”œâ”€â”€ requirements_test
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â””â”€â”€ test_schema.py
â”‚
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â””â”€â”€ READ_ME.md
```

---

# PARTIE 1 â€“ Infrastructure Docker (MinIO & PostgreSQL)

Cette partie dÃ©crit la **mise en place de lâ€™infrastructure de stockage et de persistance**, indÃ©pendante de lâ€™orchestration.

---

## 5. PrÃ©requis techniques

* Docker â‰¥ 20
* Docker Compose â‰¥ 2
* Python â‰¥ 3.9
* Git

---

## 6. DÃ©ploiement de lâ€™infrastructure

Les services **MinIO** et **PostgreSQL** sont dÃ©finis dans :

```text
docker/docker-compose.yml
```

### DÃ©marrage

```bash
cd docker
docker-compose up -d
```

---

## 7. VÃ©rifications opÃ©rationnelles

### Ã‰tat des conteneurs

```bash
docker-compose ps
```

### Logs PostgreSQL

```bash
docker-compose logs -f postgres
```

### Logs MinIO

```bash
docker-compose logs -f minio
```

---

## 8. AccÃ¨s aux services

### MinIO

* Interface Web : [http://localhost:9001](http://localhost:9001)
* Identifiants par dÃ©faut :

  * utilisateur : `minioadmin`
  * mot de passe : `secretminio`

### PostgreSQL

```bash
docker exec -it artefact-postgres psql -U postgres -d ecommerce
```

Commandes utiles :

```sql
\dn
\dt
SELECT * FROM sales.orders LIMIT 10;
```

---

## 9. ExÃ©cution manuelle du pipeline (hors Airflow)

Cette Ã©tape permet de tester le pipeline **sans orchestration**.

```bash
python ingestion/main.py 20250616
```

* Le paramÃ¨tre correspond Ã  la date dâ€™ingestion (`AAAAMMJJ`)
* Les donnÃ©es sont chargÃ©es dans le schÃ©ma `sales`

---

## 10. ArrÃªt et nettoyage

```bash
docker-compose down
docker-compose down -v   # suppression des volumes
```

---

# PARTIE 2 â€“ Orchestration avec Apache Airflow

Cette partie dÃ©crit la **mise en place et la configuration dâ€™Airflow**.

---

## 11. SÃ©curitÃ© et variables dâ€™environnement

une clÃ© frenet doit Ãªtre gÃ©nÃ©rÃ©e et ajoutÃ©e dans `docker/.env`.

### Fernet Key

```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

---

## 12. DÃ©marrage dâ€™Airflow

```bash
docker-compose -f docker/docker-compose-airflow.yml up -d
```

### AccÃ¨s Ã  lâ€™UI

* URL : [http://localhost:8080](http://localhost:8080)

---

## 13. CrÃ©ation de lâ€™utilisateur administrateur(si besoin)

sur windows remplacez \ par `

```bash
docker compose -f docker/docker-compose-airflow.yml exec airflow-apiserver \
  airflow users create \
    --username airflow \
    --password airflow \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
```

---

## 14. Configuration des connexions (UI Airflow)

ğŸ‘‰ **Toutes les connexions sont crÃ©Ã©es depuis lâ€™interface Airflow**, conformÃ©ment aux bonnes pratiques professionnelles.

### AccÃ¨s

**Admin â†’ Connections**

---

### Connexion MinIO

* Conn Id : `minio_default`
* Conn Type : `Amazon Web Services`
* Login : `minioadmin`
* Password : `secretminio`

**Extra :**

```json
{
  "endpoint_url": "http://minio:9000",
  "aws_access_key_id": "minioadmin",
  "aws_secret_access_key": "secretminio",
  "region_name": "us-east-1"
}
```

---

### Connexion PostgreSQL

* Conn Id : `postgres_ecommerce`
* Conn Type : `Postgres`
* Host : `artefact-postgres`
* Schema : `ecommerce`
* Login : `postgres`
* Password : `secretpostgres`
* Port : `5432`

---

---

## ExÃ©cution des DAGs Airflow

Ce projet contient deux DAGs principaux et un DAG test :

1. **sales_ingestion_semi-modulaire** : une version intermÃ©diaire oÃ¹ certaines tÃ¢ches sont rÃ©utilisables mais pas entiÃ¨rement dÃ©couplÃ©es.
2. **sales_ingestion_modulaire** : chaque tÃ¢che est complÃ¨tement indÃ©pendante et rÃ©utilisable, ce qui facilite la maintenance et lâ€™Ã©volution du workflow.
3. **test_minimal_airflow3** : le DAG de test 

### 1. PrÃ©requis

Avant de lancer les DAGs, assurez-vous que :

* toutes les Ã©tapes prÃ©cÃ©dentes sont dÃ©jÃ  effectuÃ©es et valides.

---

### 2. Lancer le DAG semi-modulaire

1. AccÃ©dez Ã  lâ€™interface web dâ€™Airflow : [http://localhost:8080](http://localhost:8080)
2. Recherchez le DAG nommÃ© : `sales_ingestion_semi-modulaire`.
3. Activez-le en cliquant sur le bouton **Off â†’ On**.
4. Cliquez sur **Trigger DAG** pour lancer lâ€™exÃ©cution immÃ©diate.
5. Surveillez les logs et lâ€™Ã©tat des tÃ¢ches dans la vue **Graph** ou **Tree** pour suivre lâ€™avancement.

> ğŸ”¹ Le DAG semi-modulaire conserve certaines tÃ¢ches dÃ©pendantes directement entre elles, donc les modifications de structure peuvent nÃ©cessiter un ajustement manuel.

---

### 3. Lancer le DAG modulaires

1. Dans la mÃªme interface Airflow, repÃ©rez le DAG nommÃ© : `sales_ingestion_modulaire`.
2. Activez-le (**Off â†’ On**).
3. Cliquez sur **Trigger DAG** pour dÃ©marrer lâ€™exÃ©cution.
4. Utilisez la vue **Graph** pour observer la structure complÃ¨tement modulaire et la rÃ©utilisation des tÃ¢ches.

> ğŸ”¹ GrÃ¢ce Ã  sa modularitÃ©, chaque tÃ¢che peut Ãªtre rÃ©utilisÃ©e dans dâ€™autres DAGs ou workflows sans toucher au reste du pipeline.

---

### 4. Conseils de suivi et debug

* **Logs** : toutes les sorties de tÃ¢ches sont disponibles dans la vue **Logs** pour chaque tÃ¢che.
* **Re-lancement dâ€™une tÃ¢che** : si une tÃ¢che Ã©choue, vous pouvez la **rÃ©-exÃ©cuter individuellement** sans relancer le DAG complet.
* **Version des DAGs** : assurez-vous que votre dÃ©pÃ´t est Ã  jour avant de lancer, pour Ã©viter des conflits de dÃ©pendances ou de chemins de fichiers.

---

## 15. ModÃ©lisation des donnÃ©es

```
CUSTOMER â”€â”€â”€(1,N)â”€â”€â”€ ORDER â”€â”€â”€(1,N)â”€â”€â”€ ORDER_ITEM â”€â”€â”€(N,1)â”€â”€â”€ PRODUCT
                    â”‚
                 DATE
```

* ModÃ¨le en Ã©toile
* OptimisÃ© pour lâ€™analyse
* SchÃ©ma UML : `data_analysis/Classe_UML.png`

---

## 16. Tests et qualitÃ©

```bash
pip install -r test/requirements_test
pytest test/ -v
```

---

## 17. Perspectives dâ€™Ã©volution

* DAG Airflow complet avec scheduling quotidien
* Retries, alertes et SLA
* ContrÃ´les qualitÃ© des donnÃ©es
* Monitoring et observabilitÃ©
* IntÃ©gration dbt
* SÃ©paration dev / staging / prod

---

**DerniÃ¨re mise Ã  jour : janvier 2026**

---
